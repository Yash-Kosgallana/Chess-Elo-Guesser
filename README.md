# Chess-Elo-Guesser
Data analysis of close to 30,000 chess games and model development to predict average elo of chess players given pgn.

The data was sourced from kaggle and can be found throguh the following link https://www.kaggle.com/datasets/adityajha1504/chesscom-user-games-60000-games. 
Started by using simple features such as blunders per game_lenth (eg per move). Used stockfish to evaluate the rating of each position and hence understand various features such as the average eval change per move and the variance in the eval. Eg, eval change per move should be lower in higher rated oppponents udue to making more "best moves". Added more features later on such as number of major piece moves in the first ten moves, wheter bishop pair was maintained, number of legal moves etc. For example, the position tends to get "locked" in higher rated games leading to a lower number of possible legal moves. Graphing the different types of moves (mistakes, blunders, normal) by elo showed that 2500+ rated players made more mistakes than 2300+ players. This however, is attributed to only a depth of 8 being used for stockfish, meaning higher rated players would have likely played move that stockfish at depth 8 didnt understand.

The task was seperated into two methods, one to determine the average elo of a player given the other players rating and a combined average without any prior information on either of the players elo.

Running various models, gave the mean absolute error to be only several points below the average difference, meaning a person guessing would have a similar idea to that of the model. This is due to the difficulty in predicting the rating of an indivuidal based off of one game. It is also important to note that the second method is whats done even by experts, as experts often use an opponents elo to determine a players as a game may have been submitted due to exceptional performance, as seen from Gotham Chess's Guess the Elo series.


It was also discovered that a large number of games in higher elo's ended in resignation which makes sense and that a large number of both draws and looses occured due to running out of time. Graphing hte average eval change per move also showed that the variance decreased in higher elo indicating more consistent games. A clear relationship between the number of legal moves and elo could not be seen. This indicates that number of legal moves is not an effective way of determining whether a position is closed or open. 

Attempting to solve the first method, the feature of player color existed. Running stat models regression analysis gave the coeficient of player colour to be approximately five, meaning black has a 5 elo point disadvantage. Eg, if two players are 1200, black is more like 1195 and white is 1200. However, stats models linear regression also gave the standard error of the player color coeficient to be 1.8, meaning it could be a little higher or a little less. It gave the coeficient of the result to be 65, blunders to be -6.9 and mistakes to be -7.61. Average eval change was also given the coeficient of -4.275 alighining with the earlier hypothesis.

The type of opening was also included as a feature. The common openings without proper continuation such as e4 and d4 resulted in a negative coeficient, with -59 and -56 respectively. However, less common openings such as french defense and petrovs defence had high positive coeficients. Indicating that these openings are suited for higher elo gameplay. Suprisingly however, Caro Kahn defence also has a negative coeficient of -33 however, its std error is given as 10, meaning the actual could be a lot less negative. Mistakes per move was set at -442 while blunders per move was set at -772, indicating that blunders significantly lower the average elo. Using regression foresst's feature importance tool, the most important feature was given as blunders per move, normal total and time per move in that order. Time per move however had a negative coeficient on OLS linear regression. This could be as tougher players played longer games which results in them running out of time towards the end and being forced to make a large number of moves quickly.

The second method seemed to be more useful in that a person would underpreform against the model. This is as the final model was able to predict to 220 elo points the average rating. Various models were tested, the best two being a neural network and random forrest regressor. However, the neural network was chosen as the final model as the random forest model likely to overfit. This is while we are using a large number of trees, all the trees are likely to be highly correlated and hence work in a similar manner. The neural network uses dropout at various stages to minimise overfitting as well as l2 regularisation. To combat issues with gradient exploding and vanishing, residual units were used as well as Xavier Normal initialisation. Tensorboard was also used in the process to visulaise metrics such as val loss. 

High dropout in the neural network also allowed for the use of monto carlo simulation, to get the model to provide both a standard deviation and a mean. This was then used to create an interval of five standard deviations resulting in a 65% chance of being within the range. Using three standards gave a 50 percent chance of the guess being within the interval. Also using 3 standard deviations gave the average to be 200. Meaning that if the mean was 1500 elo, the machine would say between 1300-1700. While this might seem bad, it is important to note that experts would also struggle to determine a players rating based off of one game.
